---
draft: true
title: Cloze Deletion Detection
author: Stefan Eng
date: '2018-11-05'
slug: cloze-deletion-detection
categories:
  - machine-learning
tags:
  - python
  - keras
  - neural-networks
output:
  blogdown::html_page:
    toc: no
    fig_width: 5
    fig_height: 5
aliases:
  - /1990-1992-census-crime-prediction
bibliography: ../../static/bibtex/cloze_deletion.bib
csl: ../../static/bibtex/acm-sig-proceedings.csl     
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>A cloze deletion test is a form of language test where a sentence (or
paragraph) is given to the test taker with blanks for missing words
<span class="citation">[7]</span>. The student is expected to fill in a “correct” word in the
blanks.</p>
<p>Example from Wikipedia’s article on cloze deletion <span class="citation">[8]</span>:</p>
<blockquote>
<p>Today, I went to the ____ and bought some milk and eggs.</p>
</blockquote>
<p>Some of the possible answers to fill in would be store, market, farm,
etc.</p>
<p>Cloze deletion tests can be useful for language learners. These type of
flashcards are described in great detail in Gabriel Wyner’s book, Fluent
Forever <span class="citation">[11]</span>. The idea is to include a cloze deletion
sentence, definition, a picture, other possibly relevant information
(part of speech, conjugation, etc.). An example of these flash cards can
be seen in Figure <a href="#fig:flashcard" reference-type="ref" reference="fig:flashcard">1</a> on page .</p>
<p><img src="/post/2018-11-05-cloze-deletion-detection_files/cloze_example.png" /></p>
<p>After using this method of studying for some time, I have found that
certain sentences work better than other for remembering new vocabulary
and grammar. Long sentences tended to be difficult to remember and were
not as useful as I would tend to only look at a few words around the
missing word. Cards that had a personal association were much easier to
recall. Good definitions (simple and short but descriptive) helped as
well.</p>
<p>In this paper I explore various machine learning approaches to
predicting cloze deletion sentences from two Swedish news sources. The
goals for this paper were to answer the following questions:</p>
<ul>
<li><p>Can we predict missing word using only the words around it?</p></li>
<li><p>What sentences are good example sentences?</p>
<ul>
<li>Does length of sentence make a difference?</li>
</ul></li>
<li><p>Where are good sources to find cloze deletion sentences?</p></li>
</ul>
<p>I compare the difference between an LSTM (Long-Short term memory) neural
network with that of a Bidirectional LSTM. Later the two news sources
(described in Section <a href="#sec:datasets" reference-type="ref" reference="sec:datasets">2</a>) are compared to see which data set is easier
to predict. Then I explore tuning the dropout parameter to see how
overfitting can be improved. Finally the predictions are analyzed to see
which sentences are easy to predict.</p>
</div>
<div id="data-sets" class="section level1">
<h1>Data Sets</h1>
<div class="figure">
<img src="/post/2018-11-05-cloze-deletion-detection_files/word_count_density.png" alt="Sentence Word Count Density for 8 Sidor and Göteborgs-Posten 2013" />
<p class="caption">Sentence Word Count Density for 8 Sidor and Göteborgs-Posten
2013</p>
</div>
<p>The data from Språkbanken comes in XML form <span class="citation">[4]</span>. The two
data sets I compared were 8 Sidor (from 2002/11/14 to 2017/10/09) and
Göteborgs-Posten (from 2013). 8 Sidor and Göteborgs-Posten differ in
their goals as news sites. 8 Sidor describe itself on its website as
"en nyhetstidning på lättläst svenska." <span class="citation">[1]</span>, which means that it
is a newspaper in easy to read Swedish. We can see this reflected in
Table <a href="#tab:possum" reference-type="ref" reference="tab:possum"><span class="math display">\[tab:possum\]</span></a> on page . Göteborgs-Posten (GP) is a more
traditional <span class="citation">[10]</span>. Example sentence from the two 8 Sidor is,</p>
<p>and an example from Göteborgs-Posten is</p>
<p>We can see from this example that Göteborgs-Posten uses more complex
sentence structures and more advanced vocabulary. This is also reflected
in the word count usage as seen in
Figure <a href="#fig:word_density" reference-type="ref" reference="fig:word_density"><span class="math display">\[fig:word\_density\]</span></a>. Göteborgs-Posten has a higher density of
sentences with more words. In general, longer sentences are often more
complex grammatically.</p>
<table>
<caption>Part-of-speech Count Summary<span label="tab:possum"></span></caption>
<thead>
<tr class="header">
<th></th>
<th align="left">GP2013</th>
<th align="left">8Sidor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>JJ</td>
<td align="left">35377</td>
<td align="left">4661</td>
</tr>
<tr class="even">
<td>VB</td>
<td align="left">42729</td>
<td align="left">9215</td>
</tr>
<tr class="odd">
<td>NN</td>
<td align="left">291292</td>
<td align="left">39844</td>
</tr>
</tbody>
</table>
</div>
<div id="data-processing" class="section level1">
<h1>Data Processing</h1>
<p>For exploration of the data, the Göteborgs-Posten – Två Dagar (Two
Days) data set was use which is a smaller example to start with. The
data set contains scrambled sentences from various text issues which are
tagged with an issue id. The first step was to build a data set with the
raw sentence. Python’s built in XML package was for this step. Sentences
were joined from the <em>w</em> (word) tags and part of speech tags were
retained as well. There were occasionally multiple sentences that were
found in the same text tag. These were kept as separate sentences at
first but could be joined. Punctuation was also removed from the
sentences. The available fields from Språkbanken data sets can be seen
in
Table <a href="#tab:spraakbankmeta" reference-type="ref" reference="tab:spraakbankmeta"><span class="math display">\[tab:spraakbankmeta\]</span></a>.</p>
<table>
<caption>Språkbanken Metadata<span label="tab:spraakbankmeta"></span></caption>
<thead>
<tr class="header">
<th align="left">Ordattribut (id)</th>
<th align="left">Lokalisering: svenska</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">word</td>
<td align="left">ord</td>
</tr>
<tr class="even">
<td align="left">pos</td>
<td align="left">ordklass</td>
</tr>
<tr class="odd">
<td align="left">msd</td>
<td align="left">msd</td>
</tr>
<tr class="even">
<td align="left">lemma</td>
<td align="left">saknas</td>
</tr>
<tr class="odd">
<td align="left">lex</td>
<td align="left">Lemgram</td>
</tr>
<tr class="even">
<td align="left">sense</td>
<td align="left">betydelse</td>
</tr>
<tr class="odd">
<td align="left">prefix</td>
<td align="left">förled</td>
</tr>
<tr class="even">
<td align="left">suffix</td>
<td align="left">efterled</td>
</tr>
<tr class="odd">
<td align="left">compwf</td>
<td align="left">sammansatta ordformer</td>
</tr>
<tr class="even">
<td align="left">complemgram</td>
<td align="left">sammansatta lemgram</td>
</tr>
<tr class="odd">
<td align="left">ref</td>
<td align="left">ref</td>
</tr>
<tr class="even">
<td align="left">dephead</td>
<td align="left">dephead</td>
</tr>
<tr class="odd">
<td align="left">deprel</td>
<td align="left">dependensrelation</td>
</tr>
</tbody>
</table>
<p>For the analysis, the 8 Sidor data set was used (number of sentences was
<span class="math inline">\(n = 254,711\)</span>). Then, <span class="math inline">\(n\)</span> sentences were sampled from Göteborgs-Posten
2013. This was done so that each data set had approximately the same
number of sentences so that we can compare the results fairly.</p>
<div id="data_processing" class="section level2">
<h2>Creating Training Examples</h2>
<p>Each sentence is divided into potentially many training examples. For
each noun, adjective, or verb in a sentence a window around the word was
selected. If we let define the window <span class="math inline">\(k\)</span>, and a sentence is defined as
<span class="math inline">\(s = (w_0,\ldots, w_n)\)</span> where <span class="math inline">\(w_i\)</span> is a word in the sentence (excluding
punctuation). Then for a word <span class="math inline">\(w_i\)</span>, we use
<span class="math inline">\((w_{i-k},\ldots,w_{i-1},w_{i+1},\ldots,w_{i + k})\)</span> to try to predict
<span class="math inline">\(w_i\)</span>. The before window is pre-padded with zeros when there are not
three words found before the target word. The after window is
post-padded with zeros. For all of the experiments a window size of 3
was used to predict the words.</p>
</div>
</div>
<div id="comparing-lstm-and-bidirectional-lstm" class="section level1">
<h1>Comparing LSTM and Bidirectional LSTM</h1>
<p>The first experiment I ran was to compare the results of a standard LSTM
and Bidirection LSTM. Keras is used to create the neural networks in
this paper <span class="citation">[2]</span>. The hypothesis was that a bidirectional LSTM would
provide better results because of the natural forward and backward
context for the word. For example, compare the two sentence below that
have the same start but end very differently. As seen in
Figure <a href="#fig:rnn_brnn" reference-type="ref" reference="fig:rnn_brnn"><span class="math display">\[fig:rnn\_brnn\]</span></a>, the standard LSTM feeds the sequence in in
one direction and does not have access to the later data. The
bidirectional feeds in both directions which allows for seeing the
future data.</p>
<p>I went to the <span class="math inline">\(\rule{1cm}{0.15mm}\)</span> and bought some milk and eggs.</p>
<p>I went to the <span class="math inline">\(\rule{1cm}{0.15mm}\)</span> and swam 10 laps.</p>
<p>Using a bidirectional LSTM will allow the model to use the information
from later in the sequence which could potentially improve the
performance on sentences like this example. <span class="citation">[5]</span></p>
<p><img src="/post/2018-11-05-cloze-deletion-detection_files/RNN_BRNN.png" /></p>
<p><span id="fig:rnn_brnn" label="fig:rnn_brnn"><span class="math display">\[fig:rnn\_brnn\]</span></span></p>
<div id="sec:model_config" class="section level2">
<h2>Model Configuration</h2>
<p>For the first test, the 8 Sidor data set was used (number of sentences
was <span class="math inline">\(259,216\)</span>.) A 30% validation set was used which resulted in
<span class="math inline">\(202,687\)</span> validation samples. Keras was used to implement the neural
network <span class="citation">[2]</span>. A window size of 3 was used with <span class="math inline">\(10,000\)</span> word limit
on the vocabulary. Only verbs, adjectives, and nouns were used as the
prediction word. Any word out of vocabulary was replaced with <em>UNK</em>, and
if the out of vocabulary word was found in either the before window, the
word, or the after window the training example was discarded. This
resulted in <span class="math inline">\(472,934\)</span> training examples.</p>
<p><img src="/post/2018-11-05-cloze-deletion-detection_files/atta_sample_lstm.png" title="fig:" />
<img src="/post/2018-11-05-cloze-deletion-detection_files/atta_sample_blstm.png" title="fig:" /></p>
<p><span id="fig:compare_lstm" label="fig:compare_lstm"><span class="math display">\[fig:compare\_lstm\]</span></span></p>
<p>The models were identical apart from the LSTM layer being bidirectional
which can be seen in Figure
<a href="#fig:compare_lstm" reference-type="ref" reference="fig:compare_lstm"><span class="math display">\[fig:compare\_lstm\]</span></a>. The before window and after window are
first concatenated into a single layer. Then an embedding layer is used
with an embedding size of 100. The LSTM layer has 50 units, with the
default Keras parameters of activation being <span class="math inline">\(tanh\)</span>. There is a dropout
of <span class="math inline">\(0.1\)</span> on both of the LSTM layers. The output layer has dimension
<span class="math inline">\(10,000 + 1\)</span> (the additional <span class="math inline">\(+1\)</span> is to include the out of vocabulary
token), with a softmax activation. The models were trained with the
<em>Adam</em> optimizer with 30 epochs and batch size 64. The loss function is
categorical cross entropy. The categorical cross entropy is a sum of
each of the individual cross entropy results for each category
<span class="citation">[9]</span>.
<span class="math display">\[H(y, \hat{y}) - \frac{1}{n} \sum_{i = 1}^{n} \sum_{j = 1}^{m} y_{i,j} \log(\hat{y}_{i,j})\]</span>
Where <span class="math inline">\(y\)</span> is a vector of the true values, and <span class="math inline">\(\hat{y}\)</span> are our
predictions. We define <span class="math inline">\(n\)</span> as the number of examples, <span class="math inline">\(m\)</span> as the number
of categories and <span class="math inline">\(y_{i,j}\)</span> as the <span class="math inline">\(i\)</span>th example with category <span class="math inline">\(j\)</span>. We
only have one non-zero value of <span class="math inline">\(y_{i,j}\)</span> for each <span class="math inline">\(i\)</span>. So we can
re-write this as
<span class="math display">\[H(y, \hat{y}) - \frac{1}{n} \sum_{i = 1}^{n} y_{i,c} \log(\hat{y}_{i,c})\]</span>
where <span class="math inline">\(c\)</span> is the only non-zero category for training example <span class="math inline">\(i\)</span> since
we do not have more than one category.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p><img src="/post/2018-11-05-cloze-deletion-detection_files/blstm_compare_loss.png" title="fig:" alt="Comparing accuracy and cross-entropy loss between LSTM and Bidirectional LSTM Models" /> <img src="/post/2018-11-05-cloze-deletion-detection_files/blstm_compare_acc.png" title="fig:" alt="Comparing accuracy and cross-entropy loss between LSTM and Bidirectional LSTM Models" /></p>
<p><span id="fig:lstm_compare_loss" label="fig:lstm_compare_loss"><span class="math display">\[fig:lstm\_compare\_loss\]</span></span></p>
<p>Figure
<a href="#fig:lstm_compare_loss" reference-type="ref" reference="fig:lstm_compare_loss"><span class="math display">\[fig:lstm\_compare\_loss\]</span></a> shows the accuracy and the
categorical cross entropy for the LSTM and the Bidirectional LSTM model.
We can see the Bidirectional LSTM model performs much better than the
LSTM model on the training data. It also performs a little bit better on
the validation data. From these plots we can see that both the models
are over fitting but the Bidirectional LSTM model is over fitting more
so than the standard LSTM model.</p>
</div>
</div>
<div id="improving-overfitting" class="section level1">
<h1>Improving Overfitting</h1>
<p>One way to reduce overfitting is to use dropout. Dropout removes random
nodes from a neural network while training to prevent the network from
learning too much from the noise in the dataset <span class="citation">[6]</span>.
In the LSTM and Bidirectional LSTM comparison a dropout of <span class="math inline">\(0.1\)</span> was
used for the inputs to the LSTM. To combat overfitting, the
Bidirectional LSTM model model was compared with dropout rates <span class="math inline">\(0.2\)</span></p>
<div id="dropout-comparison" class="section level2">
<h2>Dropout Comparison</h2>
<div class="figure">
<img src="/post/2018-11-05-cloze-deletion-detection_files/dropout_compare_plot.png" alt="Comparing dropout values of 0.2, 0.4, 0.6 and 0.8 on Bidirectional LSTM Model" />
<p class="caption">Comparing dropout values of 0.2, 0.4, 0.6 and 0.8 on Bidirectional
LSTM Model</p>
</div>
<p><span id="fig:dropout_compare_loss" label="fig:dropout_compare_loss"><span class="math display">\[fig:dropout\_compare\_loss\]</span></span></p>
<p>Using the Bidirectional LSTM model described in
Section <a href="#sec:model_config" reference-type="ref" reference="sec:model_config">4.1</a>, an experiment was set up to see how
dropout parameter affected the results on the model. All models has the
Bidirectional LSTM Layer configured with the dropout set to the value
<span class="math inline">\(d = 0.2, 0.4, 0.6, 0.8\)</span>. The models were all run with 25 epochs
(<span class="math inline">\(0,\ldots,24\)</span>) that took about 300 seconds for each epoch. The
recurrent dropout is also set to the same <span class="math inline">\(d\)</span> value. As described in
<span class="citation">[3]</span>, the recurrent dropout randomly drops recurrent
connection within the LSTM. The normal dropout parameter randomly drops
the inputs and output into and out of the LSTM layer.</p>
<table>
<caption>Comparing minimum loss across different dropout parameters</caption>
<thead>
<tr class="header">
<th align="right">Dropout</th>
<th align="right">Minimum Loss</th>
<th align="right">Minimum Loss Epoch</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.2</td>
<td align="right">3.374</td>
<td align="right">7</td>
</tr>
<tr class="even">
<td align="right">0.4</td>
<td align="right">3.342</td>
<td align="right">23</td>
</tr>
<tr class="odd">
<td align="right">0.6</td>
<td align="right">3.451</td>
<td align="right">23</td>
</tr>
<tr class="even">
<td align="right">0.8</td>
<td align="right">3.824</td>
<td align="right">24</td>
</tr>
</tbody>
</table>
<p><span id="tab:dropout_loss" label="tab:dropout_loss"><span class="math display">\[tab:dropout\_loss\]</span></span></p>
<p>We can see that with dropout set to <span class="math inline">\(0.2\)</span>, that the difference between
the validation loss and the training loss is very high. The validation
loss also starts to increase after epoch 7. The minimum value achieve
was at epoch 7, with a categorical cross entropy of 3.374. For the other
dropout values 0.4, 0.6, and 0.8 the minimum loss was 3.342, 3.451, and
3.824. The results are summarized in
Table <a href="#tab:dropout_loss" reference-type="ref" reference="tab:dropout_loss"><span class="math display">\[tab:dropout\_loss\]</span></a> on page .</p>
<p>We can see as the dropout value increases, the training loss and
validation loss are closer together, indicating less overfitting. When
using a higher value of dropout, the model tends to converge slower.
That is, we need more epochs to reach the same loss level. Both dropout
of <span class="math inline">\(0.6\)</span> and <span class="math inline">\(0.8\)</span>, but especially dropout of <span class="math inline">\(0.8\)</span> could have been run
for much longer to see where the loss converges to.</p>
</div>
</div>
<div id="prediction-analysis" class="section level1">
<h1>Prediction Analysis</h1>
<p>The model we trained on 8 Sidor data was used to find sentences that the
model predicted very well. First the model was used to predict the
missing values. Then the cross entropy was computed for each training
example.</p>
<p>Figure
<a href="#fig:word_count_entropy" reference-type="ref" reference="fig:word_count_entropy"><span class="math display">\[fig:word\_count\_entropy\]</span></a> shows the average (mean) cross
entropy for each word count group. The points <span class="math inline">\((x,y)\)</span> represent the mean
cross entropy of each sentence that has a word count of <span class="math inline">\(x\)</span>. The bars
represent the standard deviation within the each word count group. We
can see from this graph that very short sentences are hard to predict
(<span class="math inline">\(x &lt; 5\)</span>) as well as large sentences (<span class="math inline">\(x &gt; 25\)</span>).</p>
<div class="figure">
<img src="/post/2018-11-05-cloze-deletion-detection_files/cross_entropy_word_count.png" alt="Average Cross Entropy for each word count group" />
<p class="caption">Average Cross Entropy for each word count
group</p>
</div>
<p><span id="fig:word_count_entropy" label="fig:word_count_entropy"><span class="math display">\[fig:word\_count\_entropy\]</span></span></p>
<div id="best-prediction-examples" class="section level2">
<h2>Best Prediction Examples</h2>
<table>
<caption>Best prediction examples for 8 Sidor data set</caption>
<thead>
<tr class="header">
<th align="left">Word</th>
<th align="left">Sentence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">initiativ</td>
<td align="left">Partiet Feministiskt initiativ ställer upp i valet till EUs riksdag , Europaparlamentet .</td>
</tr>
<tr class="even">
<td align="left">eurovision</td>
<td align="left">Sanna Nielsen från Sverige sjöng i musiktävlingen Eurovision Song Contest , ESC , på tisdagen .</td>
</tr>
<tr class="odd">
<td align="left">champions</td>
<td align="left">Malmö FF förlorade även den andra matchen mot Juventus i Champions League i fotboll .</td>
</tr>
<tr class="even">
<td align="left">fredspris</td>
<td align="left">Liu Xiaobo från Kina får Nobels fredspris i år .</td>
</tr>
<tr class="odd">
<td align="left">eld</td>
<td align="left">Men muslimska ledare tror att någon tänt eld på huset .</td>
</tr>
<tr class="even">
<td align="left">tv</td>
<td align="left">Rättegången kommer att sändas i TV 4 plus .</td>
</tr>
<tr class="odd">
<td align="left">bin</td>
<td align="left">Usama bin Ladin är</td>
</tr>
<tr class="even">
<td align="left">förenta</td>
<td align="left">Förenta Nationernas organisation Barnfonden säger att det finns sextusen barnsoldater i Sudan i Afrika .</td>
</tr>
<tr class="odd">
<td align="left">green</td>
<td align="left">Höjdhopparen Emma Green Tregaro har också en bra chans att ta medalj .</td>
</tr>
<tr class="even">
<td align="left">sos</td>
<td align="left">Emil ringde till SOS Alarm för att bli hämtad av en ambulans .</td>
</tr>
<tr class="odd">
<td align="left">för</td>
<td align="left">Centrum för lättläst får pengar av staten för att göra det .</td>
</tr>
<tr class="even">
<td align="left">vicepresident</td>
<td align="left">Det säger USAs vicepresident Joe Biden .</td>
</tr>
<tr class="odd">
<td align="left">real</td>
<td align="left">Kampen står mellan Ronaldo från Real Madrid och Lionel Messi eller Andres Iniesta från Barcelona , tror experterna .</td>
</tr>
<tr class="even">
<td align="left">daglig</td>
<td align="left">Daglig verksamhet är inte ett jobb som du får lön för att göra .</td>
</tr>
<tr class="odd">
<td align="left">röda</td>
<td align="left">Men nu stoppar både Röda Korset och FN hjälpen till de människor som är fast i Aleppo .</td>
</tr>
<tr class="even">
<td align="left">fängelse</td>
<td align="left">Han är misstänkt för spioneri och kan dömas till livstids fängelse i USA .</td>
</tr>
<tr class="odd">
<td align="left">procent</td>
<td align="left">Bland eleverna är Miljöpartiet tredje största parti med nästan 15 procent av rösterna .</td>
</tr>
<tr class="even">
<td align="left">meter</td>
<td align="left">Susanna Kallur vann 100 meter häck vid en gala i Karlstad på onsdagen .</td>
</tr>
<tr class="odd">
<td align="left">butikskedjan</td>
<td align="left">Fabian Bengtsson är chef för butikskedjan Siba som säljer elektronik .</td>
</tr>
<tr class="even">
<td align="left">alarm</td>
<td align="left">Företaget SOS Alarm har fått hård kritik den senaste tiden .</td>
</tr>
</tbody>
</table>
<p><span id="tab:pred_examples" label="tab:pred_examples"><span class="math display">\[tab:pred\_examples\]</span></span></p>
<p>In
Table <a href="#tab:pred_examples" reference-type="ref" reference="tab:pred_examples"><span class="math display">\[tab:pred\_examples\]</span></a> on page , we can see the predictions from
the model which had the lowest cross entropy. Many of these top
predicted words are parts of proper nouns or named entities which is
fairly obvious because these words don’t appear in other contexts on
their own.</p>
<p>Some notable example from this list that would be good cloze deletion
example are: <em>initiativ, eld, fängelse, procent, meter</em>.</p>
<ul>
<li><p>Partiet Feministiskt <strong>initiativ</strong> ställer upp i valet till EUs
riksdag, Europaparlamentet.</p></li>
<li><p>Men muslimska ledare tror att någon tänt <strong>eld</strong> på huset. (Good in
the sense that goes together frequently)</p></li>
<li><p>Han är misstänkt för spioneri och kan dömas till livstids
<strong>fängelse</strong> i USA.</p></li>
<li><p>Bland eleverna är Miljöpartiet tredje största parti med nästan 15
<strong>procent</strong> av rösterna.</p></li>
<li><p>Susanna Kallur vann 100 <strong>meter</strong> häck vid en gala i Karlstad på
onsdagen.</p></li>
</ul>
<p>For future work, removing these named entities would potentially be
better for a language learner. We can also see from these example
sentences that when named entities are found within the window that the
predictions are very high. For example, for predicting <em>initiativ</em>, the
proceeding words <em>Partiet Feministiskt</em> are likely not seen anywhere
else in the data set. These type of examples can be good for a learner
that has a connection in some way to <em>Partiet Feministiskt</em>, to learn
the word for <em>initiativ</em>.</p>
</div>
</div>
<div id="comparing-gp2013-and-8-sidor" class="section level1">
<h1>Comparing GP2013 and 8 Sidor</h1>
<p>To create a fair comparison, a tokenizer was fit on the joined text from
both data sets. The GP2013 data set is an sample equal in size to the
full 8 Sidor data set. The vocabulary size was set to <span class="math inline">\(10,000\)</span>. For this
comparison <span class="math inline">\(n = 44,981\)</span>, validating on <span class="math inline">\(19,278\)</span>. The reason this number
is so much smaller when training only on 8 Sidor data is that the
vocabulary is now much larger. As described in section
<a href="#data_processing" reference-type="ref" reference="data_processing">3.1</a>, the training examples are not used if
there is an <em>UNK</em> token found in the window or the prediction word.
Since the total number of words in the vocabulary is larger, there will
be many more words excluded. This results in a total smaller number of
examples. While this is a rather small amount of data it is the same for
both data sets so we can compare the results between them. The model is
configured the same as described in
<a href="#sec:model_config" reference-type="ref" reference="sec:model_config">4.1</a>, but with dropout set to <span class="math inline">\(0.2\)</span>. The
tokenizer was the same for each data set but the models are trained
individually. The data was fit with 30 epochs.</p>
<div id="results-1" class="section level2">
<h2>Results</h2>
<p><img src="/post/2018-11-05-cloze-deletion-detection_files/compare_atta_gp_loss.png" alt="Comparing accuracy and cross-entropy loss between 8 Sidor and GP 2013" /> <img src="/post/2018-11-05-cloze-deletion-detection_files/compare_atta_gp_acc.png" alt="Comparing accuracy and cross-entropy loss between 8 Sidor and GP 2013" /></p>
<p><span id="fig:atta_gp2013_compare" label="fig:atta_gp2013_compare"><span class="math display">\[fig:atta\_gp2013\_compare\]</span></span></p>
<p>We can see in
Figure <a href="#fig:atta_gp2013_compare" reference-type="ref" reference="fig:atta_gp2013_compare"><span class="math display">\[fig:atta\_gp2013\_compare\]</span></a> the cross entropy and the accuracy
for each of the data sets. The training loss decreasing fast for both
data sets. As seen in section
<a href="#sec:model_config" reference-type="ref" reference="sec:model_config">4.1</a> the model overfits the training data. The
loss for the Göteborgs-Posten data set actually starts to increase with
the number of epochs. Overall, the model can predict better on the 8
Sidor data set than the data from Göteborgs-Posten 2013.</p>
<p>These results are consistent with the original hypothesis. 8 Sidor’s
intention is to create simple to read new articles without complicated
sentence structure and words. Often readers of this newspaper are
learners of the Swedish language. Göteborgs-Posten wants to be
interesting to its audience, which has presumably a majority native
Swedish speakers. The writers want to write in an interesting way to
convey a message with a much broader vocabulary. This can be seen in the
part of speech count summary found in
Table <a href="#tab:possum" reference-type="ref" reference="tab:possum"><span class="math display">\[tab:possum\]</span></a> on page .</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>In the paper I explored how we can predict cloze deletion sentences
using recurrent (LSTM) neural networks. The bidirectional variant of
LSTM was compared with a standard LSTM model. The dropout parameter was
tuned through experiments on the 8 Sidor data set.</p>
<p>The top predicted example sentences showed that proper nouns and named
entities were the easiest for the network to predict. The length of the
sentence did not play that much of a role, which is likely due to the
window size selected. We showed that when comparing Göteborgs-Posten and
8 Sidor that the neural network had an easier time predicting the
sentences from 8 Sidor. Using other preprocessing methods would be the
greatest improvement to the process.</p>
<p>While the results were not great in predicting, there is much potential
for improve the model and exploring cloze deletion in more detail.
Future work could include dictionary definitions as input the the neural
network. Other hyperparameters could be tuned to make the network
predict better. The training example creation stage could also be
improved and filtering for named entities and proper noun would make the
example sentences better. Using other data sources than news sources
would potentially give better example sentences.</p>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-8sidor">
<p>[1] 8 Sidor 2018. 8 sidor. <a href="http://8sidor.se">http://8sidor.se</a>.</p>
</div>
<div id="ref-keras">
<p>[2] Chollet, F. and others 2015. Keras. <a href="https://keras.io">https://keras.io</a>.</p>
</div>
<div id="ref-recurrent_dropout">
<p>[3] Gal, Y. and Ghahramani, Z. 2016. A theoretically grounded application of dropout in recurrent neural networks. <em>Proceedings of the 30th international conference on neural information processing systems</em> (USA, 2016), 1027–1035.</p>
</div>
<div id="ref-spraakbanken">
<p>[4] Göteborgs universitet 2018. Språkbanken. <a href="https://spraakbanken.gu.se">https://spraakbanken.gu.se</a>.</p>
</div>
<div id="ref-ngcoursera">
<p>[5] Ng, A. 2018. Sequence models – Coursera. <a href="https://www.coursera.org/learn/nlp-sequence-models">https://www.coursera.org/learn/nlp-sequence-models</a>.</p>
</div>
<div id="ref-dropout_srivastava">
<p>[6] Srivastava, N. et al. 2014. Dropout: A simple way to prevent neural networks from overfitting. <em>Journal of Machine Learning Research</em>. 15, (2014), 1929–1958.</p>
</div>
<div id="ref-clozeproc">
<p>[7] Taylor, W.L. 1953. “Cloze procedure”: A new tool for measuring readability. <em>Journalism Bulletin</em>. 30, 4 (1953), 415–433. DOI:<a href="https://doi.org/10.1177/107769905303000401">https://doi.org/10.1177/107769905303000401</a>.</p>
</div>
<div id="ref-wiki:clozetest">
<p>[8] Wikipedia contributors 2018. Cloze test — Wikipedia, the free encyclopedia. <a href="https://en.wikipedia.org/w/index.php?title=Cloze_test&amp;oldid=847324700">https://en.wikipedia.org/w/index.php?title=Cloze_test&amp;oldid=847324700</a>.</p>
</div>
<div id="ref-wiki:crossentropy">
<p>[9] Wikipedia contributors 2018. Cross entropy — Wikipedia, the free encyclopedia.</p>
</div>
<div id="ref-wiki:gp">
<p>[10] Wikipedia contributors 2018. Göteborgs-posten — Wikipedia, the free encyclopedia. <a href="https://en.wikipedia.org/w/index.php?title=G%C3%B6teborgs-Posten&amp;oldid=865127708">https://en.wikipedia.org/w/index.php?title=G%C3%B6teborgs-Posten&amp;oldid=865127708</a>.</p>
</div>
<div id="ref-fluentforever">
<p>[11] Wyner, G. 2014. <em>Fluent forever: How to learn any language fast and never forget it</em>. Harmony Books.</p>
</div>
</div>
</div>
</div>
