---
draft: false
title: Introduction to Time Series
author: Stefan Eng
date: '2019-07-06'
slug: introduction-to-time-series
categories:
  - statistics
tags:
  - R
  - time-series
output:
  blogdown::html_page:
    toc: yes
    fig_width: 5
    fig_height: 5
---


<div id="TOC">
<ul>
<li><a href="#classical-decomposition-model">Classical decomposition model</a></li>
<li><a href="#time-series-analysis">Time Series Analysis</a></li>
<li><a href="#best-linear-predictor">Best Linear Predictor</a><ul>
<li><a href="#example">Example</a></li>
</ul></li>
<li><a href="#armapq">ARMA(p,q)</a></li>
<li><a href="#causality">Causality</a><ul>
<li><a href="#finding-causal-representation">Finding Causal Representation</a></li>
<li><a href="#finding-invertible-arinfty-representation">Finding invertible (AR(<span class="math inline">\(\infty\)</span>)) representation</a></li>
</ul></li>
<li><a href="#model-building-for-arma-processes">Model Building for ARMA processes</a></li>
<li><a href="#autoregressive-processes---arp">Autoregressive Processes - AR(p)</a><ul>
<li><a href="#ar1-example">AR(1) Example</a></li>
<li><a href="#ar2">AR(2)</a></li>
</ul></li>
<li><a href="#moving-average-processes---maq">Moving Average Processes - MA(q)</a><ul>
<li><a href="#ma2-example">MA(2) example</a></li>
</ul></li>
<li><a href="#armapq-1">ARMA(p,q)</a><ul>
<li><a href="#acf-and-pacf">ACF and PACF</a></li>
</ul></li>
<li><a href="#innovations-algorithm">Innovations Algorithm</a></li>
<li><a href="#garcharch">GARCH/ARCH</a><ul>
<li><a href="#conditional-maximum-likelihood">Conditional Maximum Likelihood</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#references">References</a></li>
<li><a href="#reproducibility">Reproducibility</a></li>
</ul></li>
</ul>
</div>

<div id="classical-decomposition-model" class="section level2">
<h2>Classical decomposition model</h2>
<p><span class="math display">\[
X_t = m_t + s_t + y_y
\]</span>
where
- <span class="math inline">\(m : \mathbb{Z} \rightarrow \mathbb{R}\)</span> is a slowly changing function, the trend component.
- <span class="math inline">\(s : \mathbb Z \rightarrow \mathbb R\)</span> is a function with a known period <span class="math inline">\(d\)</span>, i.e., <span class="math inline">\(s_{t + d} = s_t\)</span> for all <span class="math inline">\(t \in \mathbb Z\)</span> and <span class="math inline">\(\sum_{j = 1}^d s_j = 0\)</span>, is called the seasonal component.
- <span class="math inline">\((y_t, t \in \mathbb Z)\)</span> is a stationary stochastic process.</p>
</div>
<div id="time-series-analysis" class="section level2">
<h2>Time Series Analysis</h2>
<ul>
<li>Always plot the data first
<ul>
<li>If there are clear sections in the data, it might be good to analyze each section separately</li>
</ul></li>
</ul>
</div>
<div id="best-linear-predictor" class="section level2">
<h2>Best Linear Predictor</h2>
<p>Let <span class="math inline">\((X_t, t \in \Z)\)</span> be a time series with <span class="math inline">\(Var(X_t) &lt; \infty\)</span> for <span class="math inline">\(t \in \Z\)</span> and
<span class="math inline">\(X^n := (X_{t_1},\ldots, X_{t_n})\)</span> be a collection of random variables of the time series at <span class="math inline">\(n\)</span> different times.
Then the _best linear predictor of <span class="math inline">\(X_t\)</span> is given by
<span class="math display">\[
b_t^l(X^n) = a_0 + a_1 X_{t_n} + \cdots + a_n X_{t_1}
\]</span>
where the coefficients are determined by the linear equations</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(X_t - b_t^l(X^n)) = 0\)</span></li>
<li><span class="math inline">\(E(X_{t_j}(X_t - b_t^l(X^n))) = 0\)</span> for all <span class="math inline">\(j = 1,\ldots,n\)</span>.</li>
</ol>
<p>If X is stationary, with mean <span class="math inline">\(\mu\)</span> and autocovariance function <span class="math inline">\(\gamma\)</span>, the coefficients are determined by
<span class="math display">\[
a_0 = \mu (1 - \sum_{i = 1}^n a_i)
\]</span>
and
<span class="math display">\[
(\gamma(t_{n + 1 - j} - t_{n + 1 - i}))_{i,j = 1}^n (a_1,\ldots,a_n)^T = (\gamma(t - t_n),\ldots, \gamma(t - t_1))^T
\]</span></p>
<p>The mean square error is:</p>
<p><span class="math display">\[
MSE(b_t^l(X^n), X_t) = E[(b_t^l(X^n) - X_t)^2] = \gamma(0) - (a_1, \ldots, a_n)(\gamma(t - t_n), \ldots, \gamma(t - t_1))^T
\]</span></p>
<p>Note: when <span class="math inline">\(X^n := (X_1, \ldots, X_n)\)</span> then the coefficients <span class="math inline">\((a_0,\ldots,a_n)\)</span> for prediction of <span class="math inline">\(X_{n + h}\)</span>
and
<span class="math display">\[
(\gamma(i - j)_{i,j = 1}^n) (a_1,\ldots, a_n)^T = (\gamma(h), \ldots, \gamma(h + n - 1))^T
\]</span></p>
<div id="example" class="section level3">
<h3>Example</h3>
<p>The notation <span class="math inline">\(t_1,\ldots, t_n\)</span> was confusing to me at first.
A good example that shown how it works is if we have an <span class="math inline">\(AR(1)\)</span> process,
<span class="math display">\[
X_t - \phi_1 X_{t - 1} = Z_t
\]</span>
Assume that we observe values at <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span>, but are missing a value for <span class="math inline">\(X_2\)</span>.
We then have <span class="math inline">\(t_1 = 1\)</span>, <span class="math inline">\(t_2 = 3\)</span>, with <span class="math inline">\(n = 2\)</span>.
We want to find:
<span class="math display">\[
b_2^l(X^1,X^3) = a_0 + a_1 X_3 + a_2 X_1
\]</span>
We have that <span class="math inline">\(a_0 = 0\)</span>, since the process has mean zero.
It then follows that,</p>
<p><span class="math display">\[
\begin{aligned}
  \begin{pmatrix}
    \gamma(t_{3 + 1 - 1} - t_{3 + 1 - 1}) &amp; \gamma(t_{3 + 1 - 2} - t_{3 + 1 - 1})\\
     \gamma(t_{3 + 1 - 1} - t_{3 + 1 - 2}) &amp;\gamma(t_{3 + 1 - 1} - t_{3 + 1 - 1})
  \end{pmatrix} 
  \begin{pmatrix}
    a_1\\
    a_2
  \end{pmatrix}
  &amp;=
  \begin{pmatrix}
    \gamma(2 - t_{2})\\
    \gamma(2 - t_{1})
  \end{pmatrix}\\
  \begin{pmatrix}
    \gamma(0) &amp; \gamma(3 - 1)\\
     \gamma(3 - 1) &amp;\gamma(0)
  \end{pmatrix} 
  \begin{pmatrix}
    a_1\\
    a_2
  \end{pmatrix}
  &amp;=
  \begin{pmatrix}
    \gamma(2 - 3)\\
    \gamma(2 - 1)
  \end{pmatrix}\\
  \begin{pmatrix}
    1 &amp; \phi_1^2\\
     \phi_1^2 &amp; 1
  \end{pmatrix} 
  \begin{pmatrix}
    a_1\\
    a_2
  \end{pmatrix}
  &amp;=
  \begin{pmatrix}
    \phi_1\\
    \phi_1
  \end{pmatrix}
\end{aligned}
\]</span></p>
<p>Which leads to a solution
<span class="math display">\[
a_1 = a_2 = \frac{\phi_1}{1 + \phi_1^2}
\]</span></p>
</div>
</div>
<div id="armapq" class="section level2">
<h2>ARMA(p,q)</h2>
<p><span class="math inline">\(\{X_t\}\)</span> is an ARMA(p,q) process if <span class="math inline">\(\{X_t\}\)</span> is <em>stationary</em> and if for every <span class="math inline">\(t\)</span>,
<span class="math display">\[
X_t - \sum_{i = 1}^p \phi_i X_{t-i} = Z_t + \sum_{j = 1}^q \theta_j Z_{t - j}
\]</span>
where <span class="math inline">\(\{Z_t\} \sim WN(0, \sigma^2)\)</span> and the polynomials <span class="math inline">\((1 - \sum_{i = 1}^p \phi_i z^i)\)</span> and <span class="math inline">\((1 + \sum_{j = 1}^q \theta_j z^j)\)</span> have no common factors.</p>
</div>
<div id="causality" class="section level2">
<h2>Causality</h2>
<p>An ARMA(p,q) process <span class="math inline">\(\{X_t\}\)</span> is <em>causal</em>, if there exists constants <span class="math inline">\(\{\psi_t\}\)</span> such that
<span class="math display">\[
\sum_{j = 0}^\infty | \psi_{j} | &lt; \infty
\]</span>
and
<span class="math display">\[
X_t = \sum_{j = 0}^\infty \psi_{j} Z_{t - j}
\]</span>
for all <span class="math inline">\(t\)</span>. That is, if we can represent the ARMA(p,q) process <span class="math inline">\(\{X_t\}\)</span> as a <span class="math inline">\(MA(\infty)\)</span> process.</p>
<p>How to actually check? Use the equivalent condition:
<span class="math display">\[
\phi(z) = 1 - \sum_{i = 1}^p \phi_i z^i \not = 0 \quad \text{for all}~|z| \leq 1
\]</span>
That is, that <span class="math inline">\(\phi(z)\)</span> has no roots <em>inside</em> (or that all root are outside the unit circle).</p>
<div id="finding-causal-representation" class="section level3">
<h3>Finding Causal Representation</h3>
<p>To represent our (causal) ARMA(p,q) process in the form:
<span class="math display">\[
  X_t = \sum_{j = 0}^\infty \psi_j Z_{t - j}
\]</span>
The sequence of <span class="math inline">\(\{\psi_j\}\)</span> is determined by the relation <span class="math inline">\(\psi(z) = \sum_{j = 0}^\infty \psi_j z^j = \theta(z) / \phi(z)\)</span>, or equivalently:
<span class="math display">\[
\psi_j - \sum_{k = 1}^p \phi_k \psi_{j - k} = \theta_j, ~j = 0,1,\ldots
\]</span>
with <span class="math inline">\(\theta_0 := 1, \theta_j := 0\)</span> for j &gt; q and <span class="math inline">\(\psi_j := 0\)</span> for <span class="math inline">\(j &lt; 0\)</span>.</p>
</div>
<div id="finding-invertible-arinfty-representation" class="section level3">
<h3>Finding invertible (AR(<span class="math inline">\(\infty\)</span>)) representation</h3>
<p>To represent our invertible ARMA(p,q) process in the form:
<span class="math display">\[
Z_t = \sum_{j = 0}^\infty \pi_j X_{t - j}
\]</span>
we find <span class="math inline">\(\{\pi_j\}\)</span> by the equations
<span class="math display">\[
\pi_j + \sum_{k = 1}^q \theta_k \pi_{j - k} = -\phi_j, ~j = 0,1,\ldots,
\]</span>
where <span class="math inline">\(\phi_0 := -1,~ \phi_j := 0\)</span> for <span class="math inline">\(j &gt; p\)</span>, and <span class="math inline">\(\pi_j := 0\)</span> for <span class="math inline">\(j &lt; 0\)</span>.</p>
</div>
</div>
<div id="model-building-for-arma-processes" class="section level2">
<h2>Model Building for ARMA processes</h2>
<ul>
<li>Remove trend and seasonality until you believe the dta can be modeled as a stationary time series.</li>
<li>Identify the order of the ARMA process for the time series
<ul>
<li>Either look at ACF/PACF</li>
<li>or by fitting (using maximum likelihood or Hannan-Rissanen estimation) sucessively higher order ARMA(p,q) to the data and choosing p, q to minimize either the AICC or BIC.</li>
</ul></li>
<li>Estimate the final model using maximum likelihood</li>
<li>Compute the residuals <span class="math inline">\(\hat{R}_t\)</span> and check that they are consistent with the specified distribution and temporal covariance structure for <span class="math inline">\(Z_t\)</span>.
<ul>
<li>If they are, then the model is considered adequate for the data.</li>
</ul></li>
</ul>
</div>
<div id="autoregressive-processes---arp" class="section level2">
<h2>Autoregressive Processes - AR(p)</h2>
<p><span class="math display">\[
X_t - \sum_{j = 1}^p \phi_j X_{t - j} = Z_t
\]</span></p>
<ul>
<li>AR(p) is not necessarily stationary!</li>
</ul>
<div id="ar1-example" class="section level3">
<h3>AR(1) Example</h3>
<p><img src="/post/2019-07-06-introduction-to-time-series_files/figure-html/unnamed-chunk-1-1.png" width="480" /></p>
</div>
<div id="ar2" class="section level3">
<h3>AR(2)</h3>
<p>Can simulate using the function <code>astsa::arima.sim</code> function
<span class="math display">\[
X_t = Z_t + 0.7 \cdot X_{t - 1} + 0.2 \cdot X_{t - 2}
\]</span>
<img src="/post/2019-07-06-introduction-to-time-series_files/figure-html/unnamed-chunk-2-1.png" width="480" /><img src="/post/2019-07-06-introduction-to-time-series_files/figure-html/unnamed-chunk-2-2.png" width="480" /></p>
</div>
</div>
<div id="moving-average-processes---maq" class="section level2">
<h2>Moving Average Processes - MA(q)</h2>
<p><span class="math display">\[
X_t = Z_t + \sum_{j = 1}^q \theta_j Z_{t - j}
\]</span>
with <span class="math inline">\(Z_t \sim WN(0, \sigma^2)\)</span>. At lag greater than or equal to q, the ACF should be zero.</p>
<div id="ma2-example" class="section level3">
<h3>MA(2) example</h3>
<p><span class="math display">\[
X_t = Z_t + 0.8 \cdot Z_{t - 1} + 0.2 \cdot Z_{t - 2}
\]</span>
<img src="/post/2019-07-06-introduction-to-time-series_files/figure-html/unnamed-chunk-3-1.png" width="480" /></p>
<p>The autocorrelation function
<img src="/post/2019-07-06-introduction-to-time-series_files/figure-html/unnamed-chunk-4-1.png" width="480" /></p>
</div>
</div>
<div id="armapq-1" class="section level2">
<h2>ARMA(p,q)</h2>
<ul>
<li>The partial autocorrelation function PACF of an ARMA(p,q) process X can be thought of as the correlation between <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t + h}\)</span> when adjusting for the intervening observations <span class="math inline">\(X_{t + 1}, \ldots, X_{t + h - 1}\)</span>.
<ul>
<li>Can be used to detect seasonality: If you have monthly data and you see a big spike at <span class="math inline">\(\hat{\alpha}(12)\)</span>, it is likely that you see a periodic effect corresponding to a calendar
year, so you should remove the seasonality.</li>
</ul></li>
</ul>
<div id="acf-and-pacf" class="section level3">
<h3>ACF and PACF</h3>
<table>
<thead>
<tr class="header">
<th>Conditional Mean Model</th>
<th>ACF</th>
<th>PACF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR(p)</td>
<td>Tails off gradually</td>
<td>Cuts off after p lags</td>
</tr>
<tr class="even">
<td>MA(q)</td>
<td>Cuts off after q lags</td>
<td>Tails off gradually</td>
</tr>
<tr class="odd">
<td>ARMA(p,q)</td>
<td>Tails off gradually</td>
<td>Tails off gradually</td>
</tr>
</tbody>
</table>
<p>Source: <a href="https://se.mathworks.com/help/econ/autocorrelation-and-partial-autocorrelation.html" class="uri">https://se.mathworks.com/help/econ/autocorrelation-and-partial-autocorrelation.html</a></p>
</div>
</div>
<div id="innovations-algorithm" class="section level2">
<h2>Innovations Algorithm</h2>
<p>Used to compute the best linear predictor, <span class="math inline">\(b_{n + 1}^l(X^n)\)</span> more computationally efficiently than solving a system of <span class="math inline">\(n\)</span> linear equations.
Can be applied to all time series with finite second moments.
Assume we have a time series <span class="math inline">\((X_t, t \in \mathbb Z)\)</span> with zero mean, and finite second moment <span class="math inline">\(E[X_t^2] &lt; + \infty\)</span> for all <span class="math inline">\(t \in \mathbb Z\)</span> and covariance
<span class="math display">\[
Cov(X_i, X_j) = \kappa(i,j)
\]</span>
We denote the best linear one-step predictors as
<span class="math display">\[
\hat{X}_n := \begin{cases}
  0 &amp; \text{for n = 1}\\
  b_{n}^l(X^{n - 1}) &amp; \text{for n &gt; 1}
\end{cases}
\]</span>
and the mean squared errors as
<span class="math display">\[
v_n := MSE(\hat{X}_{n+1}, X_{n + 1}) = E[(\hat{X}_{n + 1} - X_{n + 1})^2]
\]</span></p>
<p>There exists coefficients <span class="math inline">\((\theta_{ij}, 1 \leq j \leq i \leq n)\)</span> such that the best linear predictors satisfy
<span class="math display">\[
\hat{X}_{n + 1} = \begin{cases}
  0 &amp; \text{for } n = 0\\
  \sum_{j = 1}^n \theta_{nj}(X_{n + 1 - j} - \hat{X}_{n+1-j}) &amp; \text{for } n \geq 1
\end{cases}
\]</span>
We compute the coefficients <span class="math inline">\(\theta_{n1},\ldots, \theta_{nn}\)</span> recursively from the equations
<span class="math display">\[
v_0 := \kappa(1,1)
\]</span>
and
<span class="math display">\[
\theta_{n (n - k)} := v_k^{-1} \left( 
  \kappa(n + 1, k + 1) - \sum_{j = 0}^{k - 1} \theta_{k (k - j)} \theta_{n (n - j)} v_j
\right)
\]</span></p>
</div>
<div id="garcharch" class="section level2">
<h2>GARCH/ARCH</h2>
<div id="conditional-maximum-likelihood" class="section level3">
<h3>Conditional Maximum Likelihood</h3>
<p>The MLEs <span class="math inline">\((\hat{\alpha}_0, \ldots, \hat{\alpha}_p, \hat{\beta}_1, \ldots, \hat{\alpha}_q, \hat{\theta}_Z)\)</span> are obtained by maximizing the <em>conditional likelihood function</em>
<span class="math display">\[
L(\alpha_0, \ldots, \alpha_p, \beta_1, \ldots, \alpha_q, \theta_Z) = \prod_{t = p + 1} \frac{1}{\sigma_t} f_Z \left( \frac{x_t}{\sigma_t}\right)
\]</span>
where <span class="math inline">\(f_Z\)</span> is the density of the white noise Z and <span class="math inline">\(\theta_Z\)</span> is any other parameter Z depends on (such as degrees of freedom if Z is t-distributed).</p>
<p>It <span class="math inline">\(Z \sim IIDN(0,1)\)</span> then,
<span class="math display">\[
\begin{aligned}
f_Z \left( \frac{x_t}{\sigma_t}\right) &amp;= \frac{1}{\sqrt{2\pi}} \exp\left( - \frac{X_t^2}{2 \sigma_t^2}\right)\\
L(\alpha_0, \ldots, \alpha_p, \beta_1, \ldots, \alpha_q, \theta_Z) &amp;= \prod_{t = p + 1}^n \frac{1}{\sigma_t} \frac{1}{\sqrt{2\pi}} \exp\left( - \frac{X_t^2}{2 \sigma_t^2}\right)\\
- \ln L(\alpha_0, \ldots, \alpha_p, \beta_1, \ldots, \alpha_q, \theta_Z) &amp;= \sum_{t = p + 1}^n \ln{\sigma_t} + \frac{1}{2} \ln(2\pi)as + \frac{1}{2} \frac{X_t^2}{\sigma_t^2}\\
&amp;= \frac{1}{2} \sum_{t = p + 1}^n 2 \ln{\sigma_t} + \ln(2\pi) + \frac{X_t^2}{\sigma_t^2}\\
&amp;= \frac{1}{2} \sum_{t = p + 1}^n \ln{\sigma_t^2} + \ln(2\pi) + \frac{X_t^2}{\sigma_t^2}
\end{aligned}
\]</span></p>
</div>
<div id="acknowledgements" class="section level3">
<h3>Acknowledgements</h3>
<p>This blog post was made possible thanks to:</p>
<ul>
<li><a id='cite-Xie_2017'></a>(<a href='https://github.com/rstudio/blogdown'>Xie, Hill, and Thomas, 2017</a>)</li>
<li><a id='cite-Boettiger_2017'></a>(<a href='https://CRAN.R-project.org/package=knitcitations'>Boettiger, 2017</a>)</li>
<li><a id='cite-Csardi_2018'></a>(<a href='https://CRAN.R-project.org/package=sessioninfo'>Csárdi, core, Wickham, Chang, et al., 2018</a>)</li>
</ul>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>
<a id='bib-Boettiger_2017'></a><a href="#cite-Boettiger_2017">[1]</a><cite>
C. Boettiger.
<em>knitcitations: Citations for ‘Knitr’ Markdown Files</em>.
R package version 1.0.8.
2017.
URL: <a href="https://CRAN.R-project.org/package=knitcitations">https://CRAN.R-project.org/package=knitcitations</a>.</cite>
</p>
<p>
<a id='bib-Csardi_2018'></a><a href="#cite-Csardi_2018">[2]</a><cite>
G. Csárdi, R. core, H. Wickham, W. Chang, et al.
<em>sessioninfo: R Session Information</em>.
R package version 1.1.1.
2018.
URL: <a href="https://CRAN.R-project.org/package=sessioninfo">https://CRAN.R-project.org/package=sessioninfo</a>.</cite>
</p>
<p>
<a id='bib-Xie_2017'></a><a href="#cite-Xie_2017">[3]</a><cite>
Y. Xie, A. P. Hill, and A. Thomas.
<em>blogdown: Creating Websites with R Markdown</em>.
ISBN 978-0815363729.
Boca Raton, Florida: Chapman and Hall/CRC, 2017.
URL: <a href="https://github.com/rstudio/blogdown">https://github.com/rstudio/blogdown</a>.</cite>
</p>
</div>
<div id="reproducibility" class="section level3">
<h3>Reproducibility</h3>
<pre><code>─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────
 setting  value                       
 version  R version 3.6.3 (2020-02-29)
 os       macOS Mojave 10.14.6        
 system   x86_64, darwin15.6.0        
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 ctype    en_US.UTF-8                 
 tz       Europe/Stockholm            
 date     2020-05-11                  

─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────
 package       * version date       lib source        
 assertthat      0.2.1   2019-03-21 [1] CRAN (R 3.6.0)
 bibtex          0.4.2   2017-06-30 [1] CRAN (R 3.6.0)
 blogdown        0.13    2019-06-11 [1] CRAN (R 3.6.0)
 bookdown        0.11    2019-05-28 [1] CRAN (R 3.6.0)
 cli             2.0.2   2020-02-28 [1] CRAN (R 3.6.0)
 colorspace      1.4-1   2019-03-18 [1] CRAN (R 3.6.0)
 crayon          1.3.4   2017-09-16 [1] CRAN (R 3.6.0)
 digest          0.6.25  2020-02-23 [1] CRAN (R 3.6.0)
 dplyr           0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
 evaluate        0.14    2019-05-28 [1] CRAN (R 3.6.0)
 fansi           0.4.1   2020-01-08 [1] CRAN (R 3.6.0)
 ggplot2       * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
 glue            1.3.2   2020-03-12 [1] CRAN (R 3.6.0)
 gtable          0.3.0   2019-03-25 [1] CRAN (R 3.6.0)
 htmltools       0.4.0   2019-10-04 [1] CRAN (R 3.6.0)
 httr            1.4.1   2019-08-05 [1] CRAN (R 3.6.0)
 jsonlite        1.6.1   2020-02-02 [1] CRAN (R 3.6.0)
 knitcitations * 1.0.8   2017-07-04 [1] CRAN (R 3.6.0)
 knitr           1.28    2020-02-06 [1] CRAN (R 3.6.0)
 lifecycle       0.2.0   2020-03-06 [1] CRAN (R 3.6.0)
 lubridate       1.7.4   2018-04-11 [1] CRAN (R 3.6.0)
 magrittr        1.5     2014-11-22 [1] CRAN (R 3.6.0)
 munsell         0.5.0   2018-06-12 [1] CRAN (R 3.6.0)
 pillar          1.4.3   2019-12-20 [1] CRAN (R 3.6.0)
 pkgconfig       2.0.3   2019-09-22 [1] CRAN (R 3.6.0)
 plyr            1.8.6   2020-03-03 [1] CRAN (R 3.6.0)
 purrr           0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
 R6              2.4.1   2019-11-12 [1] CRAN (R 3.6.0)
 Rcpp            1.0.3   2019-11-08 [1] CRAN (R 3.6.0)
 RefManageR      1.2.12  2019-04-03 [1] CRAN (R 3.6.0)
 rlang           0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
 rmarkdown       2.1     2020-01-20 [1] CRAN (R 3.6.0)
 scales          1.1.0   2019-11-18 [1] CRAN (R 3.6.0)
 sessioninfo   * 1.1.1   2018-11-05 [1] CRAN (R 3.6.0)
 stringi         1.4.6   2020-02-17 [1] CRAN (R 3.6.0)
 stringr         1.4.0   2019-02-10 [1] CRAN (R 3.6.0)
 tibble          2.1.3   2019-06-06 [1] CRAN (R 3.6.0)
 tidyselect      1.0.0   2020-01-27 [1] CRAN (R 3.6.0)
 withr           2.1.2   2018-03-15 [1] CRAN (R 3.6.0)
 xfun            0.12    2020-01-13 [1] CRAN (R 3.6.0)
 xml2            1.2.5   2020-03-11 [1] CRAN (R 3.6.0)
 yaml            2.2.1   2020-02-01 [1] CRAN (R 3.6.0)

[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library</code></pre>
</div>
</div>
